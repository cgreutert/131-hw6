---
title: "Homework Six: Tree-Based Models"
author: "Carly Greutert"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, include=FALSE}
library(tidyverse)
library(corrplot)
library(tidymodels)
library(ISLR)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(ranger)
```

```{r message=FALSE, include=FALSE}
pokemon_old <- read_csv('C:\\Users\\carly\\AppData\\Local\\Temp\\Temp1_homework-5.zip\\homework-5\\data\\Pokemon.csv')
```

1. 
```{r}
library(janitor)
pokemon <- clean_names(pokemon_old)
pokemon <- filter(pokemon, type_1=="Bug" | type_1=="Fire" | type_1=="Grass" | type_1=="Normal" | type_1=="Water" | type_1=="Psychic")
names <- c('type_1', 'legendary')
pokemon[,names] <- lapply(pokemon[,names], factor)
pokemon$generation <- as.factor(pokemon$generation)
set.seed(777)
pokemon_split <- initial_split(pokemon, prop = 0.80, strata = 'type_1')
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata=type_1)
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, pokemon_train) %>% 
  step_dummy(legendary) %>%
  step_dummy(generation) %>%
  step_normalize(all_predictors())
```

2.
```{r}
pokemon_train %>% 
  select(where(is.numeric), -number) %>%
  cor() %>% 
  corrplot(type = "lower")
```

I decided to only select the numeric variables in calculating correlations since I want to visualize the linear relations between each variable. I also deleted the number/id variable since it is not useful in determining the strength of a pokemon and just functions as an identifier. I notice that most variables are strongly correlated with the total variable, which makes sense that the stronger battle stats you have, the higher total strength a pokemon has. I also notice that attack and defense are correlated with special attack and defense, respectively. This also makes sense that the higher baseline attack/defense a pokemon has, the higher chance it will also be more equipped against special attacks and defenses from other pokemon. 

3.
```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")
  
tree_wf <- workflow() %>%
add_model(tree_spec %>% 
set_args(cost_complexity = tune())) %>%
add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_wf, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
autoplot(tune_res)
```

I observe that a smaller complex penalty yields a better performance for a single tree, however it looks like the peak is right before the penalty becomes too large.                           

4.
```{r}
tune_res %>% 
  collect_metrics() %>%
  arrange(desc(mean))
```
The roc_auc of my best-performing pruned decision tree on the folds is 0.6317448.               
5.
```{r}
best_roc_auc <- select_best(tune_res)

class_tree_final <- finalize_workflow(tree_wf, best_roc_auc)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint= FALSE)
```

6.
```{r}
forest_spec <- rand_forest(min_n = tune(), trees = tune(), mtry=tune()) %>%
  set_engine("ranger", importance = 'impurity') %>%
  set_mode("classification")

forest_wf <- workflow() %>%
  add_model(forest_spec) %>% 
  add_recipe(pokemon_recipe)

param_grid2 <- grid_regular(min_n(range = c(1,10)), trees(range = c(1,500)), mtry(range = c(1,8)), levels= 8)
```
The hyperparameter mtry represents the random sample of the number of predictors for each split/fold while creating the tree models that populate the forest. The hyperparameter trees represents how many trees should be included in the forest and min_n represents the number of observations in order for a node to be split/folded further. The hyperparameter mtry must be between 1 and 8 because there are only 8 predictors in our recipe. If mtry=8, the model is a bagged model. 

7.
```{r}
tune_res2 <- tune_grid(
  forest_wf, 
  resamples = pokemon_folds, 
  grid = param_grid2, 
  metrics = metric_set(roc_auc)
)
autoplot(tune_res2)
```

I notice that a fewer number of trees yields a higher roc_auc. Also, it seems like minimal node size does not influence accuracy significantly except in the case of 500 trees it changes greatly. Furthermore, it seems like 4-6 randomly selected predictors yields the highest results. 

8.
```{r}
tune_res2 %>% 
  collect_metrics() %>%
  arrange(desc(mean))
```
The roc_auc of my best-performing random forest model on the folds is 0.7371432.

9.
```{r}
best_forest <- select_best(tune_res2)

forest_fit <- finalize_model(forest_spec, best_forest)

forest_final_fit <- fit(forest_fit, type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train)

vip(forest_final_fit)
```

Generation and legendary seem to be the least important. All others seem relatively equal in importance, except for sp_attack which seems to be most important. I'm not surprised sp_attack is the most important since that seems most diverse with respect to the type of pokemon. I am not surprised that non-battle related statistics have little importance.

10.
```{r}
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wf <- workflow() %>%
  add_model(boost_spec) %>% 
  add_recipe(pokemon_recipe)

param_grid3 <- grid_regular(trees(range = c(10, 2000)), levels= 10)

tune_res3 <- tune_grid(
  boost_wf, 
  resamples = pokemon_folds, 
  grid = param_grid3, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res3)

tune_res3 %>% 
  collect_metrics() %>%
  arrange(desc(mean))
```
I notice the higher number of trees, the higher the roc_auc. The roc_auc of my best-performing boosted tree model on the folds is 0.7305239. 

11.
```{r}
fst <- as.data.frame(arrange(collect_metrics(tune_res), desc(mean))[1,4])
snd <- as.data.frame(arrange(collect_metrics(tune_res2), desc(mean))[1,6])
trd <- as.data.frame(arrange(collect_metrics(tune_res3), desc(mean))[1,4])
rtbl <- rbind(fst, snd, trd)
row.names(rtbl)<- c("pruned tree", "random forest", "boosted tree")
rtbl

wf_final <- finalize_workflow(forest_wf, best_forest)
model_final <- fit(wf_final, data = pokemon_test)

augment(model_final, new_data = pokemon_test) %>%
  roc_auc(type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Water, .pred_Grass, .pred_Normal, .pred_Psychic))

augment(model_final, new_data = pokemon_test) %>%
roc_curve(type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Water, .pred_Grass, .pred_Normal, .pred_Psychic)) %>%
autoplot()

augment(model_final, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

My random forest was my best performing model. My model was most accurate at predicting normal and water and was the worst at predicting fire and psychic. 